{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"ChatOllama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama 모델 사용\n",
    "\n",
    "한국어 잘하는 Open 모델 \n",
    "\n",
    "**참고**\n",
    "\n",
    "각 모델의 라이센스를 반드시 확인 후 사용해주세요.\n",
    "\n",
    "- EXAONE-3.5 모델(gguf): https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct-GGUF\n",
    "- gemma2-27b: https://ollama.com/library/gemma2:27b\n",
    "- EEVE-Korean-10.8B(gguf): https://huggingface.co/teddylee777/EEVE-Korean-Instruct-10.8B-v1.0-gguf\n",
    "- Qwen2.5-7B-Instruct-kowiki-qa-context(gguf): https://huggingface.co/teddylee777/Qwen2.5-7B-Instruct-kowiki-qa-gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "# Ollama 모델 지정\n",
    "llm = ChatOllama(\n",
    "    model=\"exaone\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# 프롬프트 정의\n",
    "prompt = ChatPromptTemplate.from_template(\"{topic} 에 대하여 간략히 설명해 줘.\")\n",
    "\n",
    "# 체인 생성\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 스트림 출력\n",
    "answer = chain.stream({\"topic\": \"deep learning\"})\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemma2-27b\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma2:27b\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# 주제를 기반으로 짧은 농담을 요청하는 프롬프트 템플릿을 생성합니다.\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the following question in Korean.\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    ")\n",
    "\n",
    "# LangChain 표현식 언어 체인 구문을 사용합니다.\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 체인 실행\n",
    "answer = chain.stream({\"question\": \"python 코드로 피보나치 수열을 구현해보세요.\"})\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OllamaEmbeddings 사용\n",
    "\n",
    "링크: https://ollama.com/library/bge-m3\n",
    "\n",
    "명령어\n",
    "\n",
    "`ollama pull bge-m3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# 임베딩 설정\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"bge-m3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코사인 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"안녕하세요? 반갑습니다.\"\n",
    "sentence2 = \"안녕하세요? 반갑습니다!\"\n",
    "sentence3 = \"안녕하세요? 만나서 반가워요.\"\n",
    "sentence4 = \"Hi, nice to meet you.\"\n",
    "sentence5 = \"I like to eat apples.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유사도 계산을 위한 임베딩을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sentences = [sentence1, sentence2, sentence3, sentence4, sentence5]\n",
    "embedded_sentences = embeddings.embed_documents(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(a, b):\n",
    "    return cosine_similarity([a], [b])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유사도 계산 결과는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence1 = \"안녕하세요? 반갑습니다.\"\n",
    "# sentence2 = \"안녕하세요? 반갑습니다!\"\n",
    "# sentence3 = \"안녕하세요? 만나서 반가워요.\"\n",
    "# sentence4 = \"Hi, nice to meet you.\"\n",
    "# sentence5 = \"I like to eat apples.\"\n",
    "\n",
    "for i, sentence in enumerate(embedded_sentences):\n",
    "    for j, other_sentence in enumerate(embedded_sentences):\n",
    "        if i < j:\n",
    "            print(\n",
    "                f\"[유사도 {similarity(sentence, other_sentence):.4f}] {sentences[i]} \\t <=====> \\t {sentences[j]}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**실습에 활용한 문서**\n",
    "\n",
    "소프트웨어정책연구소(SPRi) - 2023년 12월호\n",
    "\n",
    "- 저자: 유재흥(AI정책연구실 책임연구원), 이지수(AI정책연구실 위촉연구원)\n",
    "- 링크: https://spri.kr/posts/view/23669\n",
    "- 파일명: `SPRI_AI_Brief_2023년12월호_F.pdf`\n",
    "\n",
    "_실습을 위해 다운로드 받은 파일을 `data` 폴더로 복사해 주시기 바랍니다_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "# 문서 로드\n",
    "loader = PDFPlumberLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "\n",
    "# 문서 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "split_docs = loader.load_and_split(text_splitter)\n",
    "\n",
    "# 임베딩 설정\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"bge-m3\",\n",
    ")\n",
    "\n",
    "# 벡터스토어 생성\n",
    "vectorstore = FAISS.from_documents(documents=split_docs, embedding=embeddings)\n",
    "\n",
    "# 검색기 생성\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# 프롬프트 로드\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Answer in Korean.\n",
    "\n",
    "Please follow these instructions:\n",
    "\n",
    "1. Analyze the content of the source documents: \n",
    "2. The name of each source document is at the start of the document, with the <document> tag.\n",
    "\n",
    "-----\n",
    "\n",
    "Output format should be like this:\n",
    "\n",
    "(Your comprehensive answer to the question)\n",
    "\n",
    "**Source**\n",
    "- [1] Document source with page number\n",
    "- [2] Document source with page number\n",
    "(...)\n",
    "\n",
    "-----\n",
    "\n",
    "### Here is the context that you can use to answer the question:\n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "### Here is user's question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your answer to the question:\n",
    "\n",
    "### Answer:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"<document><content>{doc.page_content}</content><page>{doc.metadata['page']}</page><source>{doc.metadata['source']}</source></document>\"\n",
    "        for doc in docs\n",
    "    )\n",
    "\n",
    "\n",
    "# Ollama 모델 지정\n",
    "llm = ChatOllama(\n",
    "    model=\"exaone\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# 체인 생성\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"삼성전자가 개발한 생성형 AI 의 이름은?\"\n",
    "\n",
    "# 체인 실행\n",
    "response = chain.stream(question)\n",
    "# 스트림 출력\n",
    "stream_response(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
